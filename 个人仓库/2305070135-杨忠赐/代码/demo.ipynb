{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cae3f9",
   "metadata": {},
   "source": [
    "conda env remove d2l-zh\n",
    "conda create -n -y d2l-zh python=3.8 pip\n",
    "conda activate d2l-zh\n",
    "\n",
    "pip install -y jupyter d2l torch torchvision\n",
    "\n",
    "wget https://zh-v2.d2l.ai/d2l-zh.zip\n",
    "unzip d2l-zh.zip\n",
    "jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ef2afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 14 15 16]\n",
      "7\n",
      "[5 6 7 8]\n",
      "[ 2  6 10 14]\n",
      "[[ 6  7  8]\n",
      " [10 11 12]]\n",
      "[[ 1  3]\n",
      " [13 15]]\n"
     ]
    }
   ],
   "source": [
    "##########  访问数组里的元素\n",
    "#   一个，行列，集中、分散子区域\n",
    "import numpy as np\n",
    "a = np.array([\n",
    "  [1, 2, 3, 4],\n",
    "  [5, 6, 7, 8],\n",
    "  [9, 10, 11, 12],\n",
    "  [13, 14, 15, 16]\n",
    "])\n",
    "print(a[-1])\n",
    "print(a[1,2],a[1,:],a[:,1],a[1:3,1:],a[::3,::2],sep=\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d7f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建张量\n",
      "tensor(66) tensor(0.) tensor(24.)\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[2, 1, 4, 3],\n",
      "        [1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n",
      "tensor([[[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n",
      "tensor([[-1.1245, -1.6115, -0.3230, -1.3030],\n",
      "        [-0.7266,  1.4437, -0.0576,  0.2556],\n",
      "        [ 0.1339,  0.3285, -1.2216,  1.6717],\n",
      "        [-1.6071, -0.2764, -0.2063, -0.7472]])\n",
      "tensor([[ 1.4955, -0.3414,  1.6303,  0.1221],\n",
      "        [ 1.0471, -0.7113, -1.2715,  0.5610],\n",
      "        [ 0.2593, -0.5419,  0.7380,  0.2211],\n",
      "        [-0.5794,  0.8673,  0.4203,  1.3922]])\n",
      "张量运算\n",
      "tensor([1., 2., 4., 8.])\n",
      "tensor([2, 2, 2, 2])\n",
      "tensor([ 3.,  4.,  6., 10.])\n",
      "tensor([-1.,  0.,  2.,  6.])\n",
      "tensor([ 2.,  4.,  8., 16.])\n",
      "tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "张量连结\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "##########  创建张量\n",
    "#   arange(s), zeros((x,y,h)), ones((x,y,h)), tensor([ [2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1] ]), 逻辑运算符, randa(,), sum()\n",
    "print(\"创建张量\")\n",
    "import torch\n",
    "x1= torch.arange(12)\n",
    "x1_1  = torch.arange(12.0) # 加.0自动为float类型\n",
    "x2= torch.zeros((2,3,4))\n",
    "x3= torch.ones((2,3,4))\n",
    "x4= torch.tensor([ [2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1] ])\n",
    "x5= torch.randn(4, 4)   # 生成随机的 标准正态分布（均值 μ=0，标准差 σ=1）【数值大部分集中在 -2 ~ 2 之间（约 95%）】\n",
    "x5_1= torch.randn(4, 4, dtype=torch.float32)  # 显式指定float32\n",
    "x5_2= torch.randn(4, 4).float()\n",
    "X= x2==x3\n",
    "\n",
    "print(x1.sum(),x2.sum(), x3.sum())\n",
    "print(x1, x1_1, x2, x3, x4, X, x5, x5_1, sep=\"\\n\")\n",
    "\n",
    "\n",
    "##########  张量运算【加减乘除】\n",
    "print(\"张量运算\")\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "\n",
    "x1 = x + y\n",
    "x2 = x - y\n",
    "x3 = x * y\n",
    "x4 = x / y\n",
    "print(x, y, x1, x2, x3, x4, sep=\"\\n\")\n",
    "\n",
    "##########  张量连结\n",
    "print(\"张量连结\")\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n",
    "x5 = torch.cat((X, Y),dim=0)\n",
    "x6 = torch.cat((X, Y),dim=1)\n",
    "print(X, Y, x5, x6, sep=\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e640e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n",
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "##########  张量广播机制\n",
    "a = torch.arange(3).reshape((3,1))\n",
    "b = torch.arange(2).reshape((1,2))\n",
    "c = a+b\n",
    "print(a, b, c, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91a40445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([12])\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0],\n",
       "         [ 1],\n",
       "         [ 2],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11]]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########  访问张量的形状和元素总数,改变张量形状\n",
    "#   shape,numel()，reshape(x,y,...)、reshape((1,-1))、reshape((-1,1))\n",
    "x1 = torch.arange(12)\n",
    "print(x1,x1.shape,x1.numel(),sep=\"\\n\")\n",
    "x2 = x1.reshape(3,4)\n",
    "x3 = x1.reshape((-1,1))\n",
    "x4 = x1.reshape((1,-1))\n",
    "x2, x3, x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb3f16f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "写入张量\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor(6)\n",
      "tensor(9)\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  9,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[4, 5, 9, 7]])\n",
      "tensor([[12, 12, 12, 12]])\n",
      "转化为numpy张量\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "大小为 1 的张量转 Python 标量\n",
      "tensor([3.5000])\n",
      "3.5\n",
      "3.5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "##########  写入张量\n",
    "print(\"写入张量\")\n",
    "X = torch.arange(12).reshape((3,4))\n",
    "print(X)\n",
    "print(X[1,2])\n",
    "X[1,2] = 9\n",
    "print(X[1,2])\n",
    "\n",
    "print(X)\n",
    "print(X[1:2,:])\n",
    "X[1:2,:] = 12\n",
    "print(X[1:2,:])\n",
    "\n",
    "##########  转化为numpy张量，大小为 1 的张量转 Python 标量\n",
    "print(\"转化为numpy张量\")\n",
    "A = X.numpy()\n",
    "# 将 NumPy 数组 A 转换回 PyTorch 张量\n",
    "B = torch.tensor(A)\n",
    "# 查看两者的数据类型\n",
    "print(type(A), type(B), sep=\"\\n\")\n",
    "\n",
    "print(\"大小为 1 的张量转 Python 标量\")\n",
    "# 创建一个仅包含单个元素的张量\n",
    "a = torch.tensor([3.5])\n",
    "# 多种方式提取为 Python 标量\n",
    "print(a, a.item(), float(a), int(a), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7fbe21",
   "metadata": {},
   "source": [
    "### 第一段：验证运算后张量地址变化\n",
    "```python\n",
    "# 记录 Y 原来的内存地址\n",
    "before = id(Y)\n",
    "# 执行加法运算并重新赋值\n",
    "Y = Y + X\n",
    "# 比较运算前后 Y 的内存地址是否相同\n",
    "id(Y) == before\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 第二段：执行原地操作（保持内存地址不变）\n",
    "```python\n",
    "# 创建一个和 Y 形状、数据类型相同的全零张量 z\n",
    "z = torch.zeros_like(Y)\n",
    "# 打印 z 初始的内存地址\n",
    "print('id(z):', id(z))\n",
    "\n",
    "# 使用切片语法, 执行原地赋值，不会创建新张量\n",
    "z[:] = X + Y\n",
    "# 再次打印 z 的内存地址，验证地址未变\n",
    "print('id(z):', id(z))\n",
    "\n",
    "\n",
    "#  X += Y 也是原地操作\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04f67484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "########### 数据预处理\n",
    "#   建立文件夹（ os.makedirs(,) ）、文件（ os.path.join(,,) ）\n",
    "import os\n",
    "os.makedirs(os.path.join(\"..\",\"data\"),exist_ok=True)\n",
    "data_file = os.path.join(\"..\",\"data\",\"house_tiny.cvs\")\n",
    "with open(data_file,\"w\") as f:\n",
    "    f.write(\"NumRooms,Alley,Price\\n\")\n",
    "    f.write(\"NA,Pave,127500\\n\")\n",
    "    f.write(\"2,NA,106000\\n\")\n",
    "    f.write(\"4,NA,178100\\n\")\n",
    "    f.write(\"NA,NA,140000\\n\")\n",
    "\n",
    "#   数据加载、读取（ read_csv() ）\n",
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daceb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n",
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0         3           1          0\n",
      "1         2           0          1\n",
      "2         4           0          1\n",
      "3         3           0          1\n"
     ]
    }
   ],
   "source": [
    "##########  处理数据-【缺失数据】\n",
    "#   插值法（ .fillna( . ) ）\n",
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:,2]\n",
    "inputs = inputs.fillna( inputs.mean(numeric_only=True) )\n",
    "print(inputs)\n",
    "\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "inputs = inputs.astype(int)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f1ecc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 1, 0],\n",
      "        [2, 0, 1],\n",
      "        [4, 0, 1],\n",
      "        [3, 0, 1]], dtype=torch.int32)\n",
      "tensor([127500, 106000, 178100, 140000])\n",
      "(tensor([3.5000]), 3.5, 3.5, 3)\n"
     ]
    }
   ],
   "source": [
    "##########   inputs和outputs中所有条目都是数值类型，就可转化为张量格式\n",
    "#   数值型dataframe转化为张量\n",
    "import torch\n",
    "x,y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "print(x,y,sep=\"\\n\")\n",
    "\n",
    "# 将大小为1的张量转换为Python标量\n",
    "a = torch.tensor([3.5]) # 创建一个大小为1的张量\n",
    "result = (a, a.item(), float(a), int(a)) # 多种方式将其转为Python标量\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24a72b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15],\n",
      "         [16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23],\n",
      "         [24, 25, 26, 27],\n",
      "         [28, 29, 30, 31],\n",
      "         [32, 33, 34, 35],\n",
      "         [36, 37, 38, 39]]]) torch.Size([2, 5, 4])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15],\n",
      "         [16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 22, 24, 26],\n",
      "         [28, 30, 32, 34],\n",
      "         [36, 38, 40, 42],\n",
      "         [44, 46, 48, 50],\n",
      "         [52, 54, 56, 58]]]) torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "##########  线性代数实现\n",
    "\n",
    "#   通过指定两个分量m和n来创建一个形状为m*n的矩阵, 转置\n",
    "import torch\n",
    "x = torch.arange(20).reshape(5,4)\n",
    "# print(x, x.T, sep=\"\\n\")\n",
    "\n",
    "#   访问张量的长度\n",
    "# print( x[3], len(x), x.shape, sep=\"\\n\")\n",
    "\n",
    "#   指定轴的求和汇总张量【求平均值同理，只需sum改成mean】\n",
    "A = torch.arange(20*2).reshape(2,5,4)\n",
    "A_sum_aixs0 = A.sum(dim=0)  #  各批融合\n",
    "A_sum_aixs1 = A.sum(dim=1)  #   各行融合\n",
    "A_sum_aixs2 = A.sum(dim=2)  #   各列融合\n",
    "A_sum_aixs01 = A.sum(dim=[0,1]) #   先批融合，再行融合\n",
    "sum_A = A.sum(axis=2, keepdims=True) #计算总和或均值时保持轴数不变,要融合的轴变为1\n",
    "\n",
    "print(A, A.shape)\n",
    "# print(A_sum_aixs0, A_sum_aixs0.shape)\n",
    "# print(A_sum_aixs1, A_sum_aixs1.shape)\n",
    "# print(A_sum_aixs2, A_sum_aixs2.shape)\n",
    "# print(A_sum_aixs01, A_sum_aixs01.shape)\n",
    "# print(sum_A, sum_A.shape)\n",
    "\n",
    "#   指定轴的累积总和\n",
    "A_cumsum_axis0 = A.cumsum(axis=0)\n",
    "print(A_cumsum_axis0, A_cumsum_axis0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cecfe220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.) tensor(6.)\n",
      "==================================================\n",
      "1. 矩阵-向量乘法 (A × u) 结果：\n",
      "值： tensor([6., 6., 6., 6.])\n",
      "形状： torch.Size([4])  | 数据类型： torch.float32\n",
      "\n",
      "2. 矩阵-矩阵乘法 (A × B) 结果：\n",
      "值：\n",
      " tensor([[24., 28., 32., 36.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [24., 28., 32., 36.]])\n",
      "形状： torch.Size([4, 4])  | 数据类型： torch.float32\n",
      "\n",
      "3. 向量u的L2范数（欧几里得范数）： 3.7416574954986572\n",
      "4. 向量u的L1范数（曼哈顿范数）： 6.0\n",
      "5. 4×9全1矩阵的弗罗尼乌斯范数： 6.0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#   点积:   各相同位置的元素相乘之和\n",
    "import torch\n",
    "y = torch.ones(4, dtype= torch.float32)\n",
    "x = torch.arange(4,dtype= torch.float32)\n",
    "x_y_1 = torch.dot(x,y) # 点积 法1 dot(,)\n",
    "x_y_2 = torch.sum(x*y) # 点积 法2 先元素乘法，再进行求和 \n",
    "print(x_y_1, x_y_2)\n",
    "\n",
    "#   Ax, AB, L2范数， L1范数, 弗罗尼乌斯范数\n",
    "A = torch.ones(16, dtype=torch.float32).reshape((4,4))\n",
    "B = torch.arange(16.0).reshape((4,4))\n",
    "u = torch.arange(4.0)\n",
    "Ax_mv = torch.mv(A, x) # 两个输入必须 dtype 一致\n",
    "AB_mm = torch.mm(A, B) # 两个输入必须 dtype 一致\n",
    "u_2 = torch.norm(u)    # 输入必须dtype为浮点 / 复数型\n",
    "u_1 = torch.abs(u).sum()\n",
    "fln = torch.norm(torch.ones((4,9)))\n",
    "print(\"=\"*50)\n",
    "print(\"1. 矩阵-向量乘法 (A × u) 结果：\")\n",
    "print(\"值：\", Ax_mv)\n",
    "print(\"形状：\", Ax_mv.shape, \" | 数据类型：\", Ax_mv.dtype)\n",
    "\n",
    "print(\"\\n2. 矩阵-矩阵乘法 (A × B) 结果：\")\n",
    "print(\"值：\\n\", AB_mm)\n",
    "print(\"形状：\", AB_mm.shape, \" | 数据类型：\", AB_mm.dtype)\n",
    "\n",
    "print(\"\\n3. 向量u的L2范数（欧几里得范数）：\", u_2.item())\n",
    "print(\"4. 向量u的L1范数（曼哈顿范数）：\", u_1.item())\n",
    "print(\"5. 4×9全1矩阵的弗罗尼乌斯范数：\", fln.item())\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61205e",
   "metadata": {},
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone() # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9529b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad (初始值): None\n",
      "y: tensor(28., grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([ 0.,  4.,  8., 12.])\n",
      "x.grad == 4 * x: tensor([True, True, True, True])\n",
      "第一次梯度: tensor([ 0.,  8., 16., 24.])\n",
      "第二次梯度: tensor([1., 1., 1., 1.])\n",
      "x.grad: tensor([0., 2., 4., 6.])\n",
      "x.grad == u: tensor([True, True, True, True])\n",
      "x.grad == 2 * x: tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "########### 自动求导\n",
    "#   对非标量调用“backward”需要传入一个“gradient”参数\n",
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "\n",
    "# 定义张量并开启梯度追踪\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)  # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n",
    "print(\"x.grad (初始值):\", x.grad)  # 初始为 None\n",
    "\n",
    "# 计算 y\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "# 定义张量并开启梯度追踪\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "# 计算 y\n",
    "y = 2 * torch.dot(x, x)\n",
    "\n",
    "# 反向传播计算梯度\n",
    "y.backward()\n",
    "# 查看梯度\n",
    "print(\"x.grad:\", x.grad)\n",
    "# 验证梯度是否等于 4 * x\n",
    "print(\"x.grad == 4 * x:\", x.grad == 4 * x)\n",
    "\n",
    "# --- In [6] ---\n",
    "# 第一次计算 y = 2 * x^2\n",
    "y1 = 2 * torch.dot(x, x)\n",
    "y1.backward()\n",
    "print(\"第一次梯度:\", x.grad)\n",
    "# 清空梯度，计算新的函数 y = x.sum()\n",
    "x.grad.zero_()  # 清除之前累积的梯度\n",
    "y2 = x.sum()\n",
    "y2.backward()\n",
    "print(\"第二次梯度:\", x.grad)\n",
    "\n",
    "# --- In [7] ---\n",
    "# 清空之前累积的梯度\n",
    "x.grad.zero_()\n",
    "# 计算 y = x * x（逐元素相乘）\n",
    "y = x * x\n",
    "# 对 y 求和后反向传播，等价于 y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "# 查看梯度\n",
    "print(\"x.grad:\", x.grad)\n",
    "\n",
    "# --- In [8] ---\n",
    "# 清空梯度\n",
    "x.grad.zero_()\n",
    "# 计算 y = x * x\n",
    "y = x * x\n",
    "# 分离 y，得到 u，u 不再参与梯度计算\n",
    "u = y.detach()\n",
    "# 计算 z = u * x\n",
    "z = u * x\n",
    "# 对 z 求和后反向传播\n",
    "z.sum().backward()\n",
    "# 验证梯度是否等于 u\n",
    "print(\"x.grad == u:\", x.grad == u)  # 输出 tensor([True, True, True, True])\n",
    "\n",
    "# --- In [9] ---\n",
    "# 清空梯度\n",
    "x.grad.zero_()\n",
    "# 对 y 求和后反向传播\n",
    "y.sum().backward()\n",
    "# 验证梯度是否等于 2 * x\n",
    "print(\"x.grad == 2 * x:\", x.grad == 2 * x)  # 输出 tensor([True, True, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92786e",
   "metadata": {},
   "source": [
    "即使构建函数的计算图需要通过 Python 控制流（如 while 循环和 if 条件或任意函数调用），我们仍然可以计算得到变量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd9de307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad: tensor(2048.)\n",
      "a.grad == d / a: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- In [10] ---\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "# 创建标量张量 a，并开启梯度追踪\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "# 计算 d = f(a)\n",
    "d = f(a)\n",
    "# 反向传播计算梯度\n",
    "d.backward()\n",
    "\n",
    "# 查看梯度\n",
    "print(\"a.grad:\", a.grad)\n",
    "# 验证梯度是否等于 d / a\n",
    "print(\"a.grad == d / a:\", a.grad == d / a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
